name: Run Maven

on:
  workflow_call:
    inputs:
      java_version:
        description: 'The Java version to use'
        required: false
        type: string
        default: '17'
      maven_args:
        description: 'The Maven goals to execute'
        required: false
        type: string
        default: 'clean package --file pom.xml'
      test-report:
        description: 'Whether to generate test reports'
        required: false
        type: boolean
        default: false
      summaries-test-report:
        description: 'Whether to generate test summaries'
        required: false
        type: boolean
        default: false
    secrets:
      token:
        required: true
jobs:
  build:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
        env:
          GITHUB_TOKEN: ${{ secrets.token }}

      - name: Set up JDK
        uses: actions/setup-java@v4
        with:
          java-version: ${{ inputs.java_version }}
          distribution: zulu
          cache: maven

      - name: Cache Maven dependencies
        uses: actions/cache@v4
        with:
          path: ~/.m2/repository
          key: maven-${{ runner.os }}-${{ hashFiles('**/pom.xml') }}
          restore-keys: |
            maven-${{ runner.os }}-

      - name: Build with Maven
        run: mvn ${{ inputs.maven_args }}
      
      - name: Generate aggregated Surefire reports
        if: ${{ inputs.test-report || inputs.summaries-test-report }}
        run: |
          # Create directory for aggregated reports
          mvn surefire-report:report-only -B
        
      - name: Parse test results and add to summary
        if: ${{ inputs.summaries-test-report }}
        run: |
          # Create workflow summary with test results
          echo "# 📊 Test Report Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Count total tests from XML files
          TOTAL_TESTS=0
          TOTAL_FAILURES=0
          TOTAL_ERRORS=0
          TOTAL_SKIPPED=0
          
          # Process all TEST-*.xml files from all modules
          for xml_file in */target/surefire-reports/TEST-*.xml; do
            if [ -f "$xml_file" ]; then
              tests=$(grep -o 'tests="[0-9]*"' "$xml_file" | grep -o '[0-9]*' || echo "0")
              failures=$(grep -o 'failures="[0-9]*"' "$xml_file" | grep -o '[0-9]*' || echo "0")
              errors=$(grep -o 'errors="[0-9]*"' "$xml_file" | grep -o '[0-9]*' || echo "0")
              skipped=$(grep -o 'skipped="[0-9]*"' "$xml_file" | grep -o '[0-9]*' || echo "0")
              
              TOTAL_TESTS=$((TOTAL_TESTS + tests))
              TOTAL_FAILURES=$((TOTAL_FAILURES + failures))
              TOTAL_ERRORS=$((TOTAL_ERRORS + errors))
              TOTAL_SKIPPED=$((TOTAL_SKIPPED + skipped))
            fi
          done
          
          TOTAL_PASSED=$((TOTAL_TESTS - TOTAL_FAILURES - TOTAL_ERRORS - TOTAL_SKIPPED))
          
          # Create summary table
          echo "## Test Results Overview" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Status | Count | Percentage |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|-------|------------|" >> $GITHUB_STEP_SUMMARY
          
          if [ $TOTAL_TESTS -gt 0 ]; then
            PASS_PCT=$(( (TOTAL_PASSED * 100) / TOTAL_TESTS ))
            FAIL_PCT=$(( ((TOTAL_FAILURES + TOTAL_ERRORS) * 100) / TOTAL_TESTS ))
            SKIP_PCT=$(( (TOTAL_SKIPPED * 100) / TOTAL_TESTS ))
            
            echo "| ✅ **Passed** | $TOTAL_PASSED | ${PASS_PCT}% |" >> $GITHUB_STEP_SUMMARY
            echo "| ❌ **Failed** | $((TOTAL_FAILURES + TOTAL_ERRORS)) | ${FAIL_PCT}% |" >> $GITHUB_STEP_SUMMARY
            echo "| ⏭️ **Skipped** | $TOTAL_SKIPPED | ${SKIP_PCT}% |" >> $GITHUB_STEP_SUMMARY
            echo "| 📊 **Total** | $TOTAL_TESTS | 100% |" >> $GITHUB_STEP_SUMMARY
          else
            echo "| ⚠️ **No tests found** | 0 | 0% |" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Add module breakdown
          echo "## Module Test Breakdown" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Module | Tests | Passed | Failed | Skipped |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|-------|--------|--------|---------|" >> $GITHUB_STEP_SUMMARY
          
          for module in api-gateway api-registry api-registry-common; do
            if [ -d "$module/target/surefire-reports" ]; then
              MODULE_TESTS=0
              MODULE_FAILURES=0
              MODULE_ERRORS=0
              MODULE_SKIPPED=0
              
              for xml_file in $module/target/surefire-reports/TEST-*.xml; do
                if [ -f "$xml_file" ]; then
                  tests=$(grep -o 'tests="[0-9]*"' "$xml_file" | grep -o '[0-9]*' || echo "0")
                  failures=$(grep -o 'failures="[0-9]*"' "$xml_file" | grep -o '[0-9]*' || echo "0")
                  errors=$(grep -o 'errors="[0-9]*"' "$xml_file" | grep -o '[0-9]*' || echo "0")
                  skipped=$(grep -o 'skipped="[0-9]*"' "$xml_file" | grep -o '[0-9]*' || echo "0")
                  
                  MODULE_TESTS=$((MODULE_TESTS + tests))
                  MODULE_FAILURES=$((MODULE_FAILURES + failures))
                  MODULE_ERRORS=$((MODULE_ERRORS + errors))
                  MODULE_SKIPPED=$((MODULE_SKIPPED + skipped))
                fi
              done
              
              MODULE_PASSED=$((MODULE_TESTS - MODULE_FAILURES - MODULE_ERRORS - MODULE_SKIPPED))
              MODULE_FAILED=$((MODULE_FAILURES + MODULE_ERRORS))
              
              if [ $MODULE_TESTS -gt 0 ]; then
                echo "| **$module** | $MODULE_TESTS | $MODULE_PASSED | $MODULE_FAILED | $MODULE_SKIPPED |" >> $GITHUB_STEP_SUMMARY
              fi
            fi
          done
          
          # Set output variables for potential use in other jobs
          echo "total_tests=$TOTAL_TESTS" >> $GITHUB_OUTPUT
          echo "total_passed=$TOTAL_PASSED" >> $GITHUB_OUTPUT
          echo "total_failed=$((TOTAL_FAILURES + TOTAL_ERRORS))" >> $GITHUB_OUTPUT
          echo "total_skipped=$TOTAL_SKIPPED" >> $GITHUB_OUTPUT
        
      - name: Comment test summary on PR
        if: ${{ inputs.summaries-test-report && github.event_name == 'pull_request' }}
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.token }}
          script: |
            const fs = require('fs');
            
            // Read test results from output file
            const outputFile = process.env.GITHUB_OUTPUT;
            const outputContent = fs.readFileSync(outputFile, 'utf8');
            
            const getOutputValue = (key) => {
              const match = outputContent.match(new RegExp(`${key}=(\\d+)`));
              return match ? parseInt(match[1]) : 0;
            };
            
            const totalTests = getOutputValue('total_tests');
            const totalPassed = getOutputValue('total_passed');
            const totalFailed = getOutputValue('total_failed');
            const totalSkipped = getOutputValue('total_skipped');
            
            // Create summary comment
            let comment = `## 📊 Test Results Summary\n\n`;
            
            if (totalTests === 0) {
              comment += `⚠️ **No tests were found or executed.**\n\n`;
            } else {
              const passPercent = Math.round((totalPassed / totalTests) * 100);
              const failPercent = Math.round((totalFailed / totalTests) * 100);
              const skipPercent = Math.round((totalSkipped / totalTests) * 100);
              
              // Overall status
              const overallStatus = totalFailed === 0 ? '✅ **All tests passed!**' : `❌ **${totalFailed} test(s) failed**`;
              comment += `${overallStatus}\n\n`;
              
              // Results table
              comment += `| Status | Count | Percentage |\n`;
              comment += `|--------|-------|------------|\n`;
              comment += `| ✅ **Passed** | ${totalPassed} | ${passPercent}% |\n`;
              comment += `| ❌ **Failed** | ${totalFailed} | ${failPercent}% |\n`;
              comment += `| ⏭️ **Skipped** | ${totalSkipped} | ${skipPercent}% |\n`;
              comment += `| 📊 **Total** | ${totalTests} | 100% |\n\n`;
              
              // Additional details
              if (totalFailed > 0) {
                comment += `> 🔍 **Action Required**: Please review and fix the failing tests before merging.\n\n`;
              }
            }
            
            comment += `*Generated by Maven Test Workflow*`;
            
            // Post comment to PR
            await github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
        
      - name: Upload Surefire reports
        if: ${{ inputs.test-report }}
        uses: actions/upload-artifact@v4
        with:
          name: unit-test-report
          path: target/reports/
          retention-days: 30
          
      - name: Upload individual module reports
        if: ${{ inputs.test-report }}
        uses: actions/upload-artifact@v4
        with:
          name: unit-test-modules-report
          path: |
            */target/site/surefire-report.html
            */target/surefire-reports/
          retention-days: 30
