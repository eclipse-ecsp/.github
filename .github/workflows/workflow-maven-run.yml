name: Run Maven

on:
  workflow_call:
    inputs:
      java_version:
        description: 'The Java version to use'
        required: false
        type: string
        default: '17'
      maven_args:
        description: 'The Maven goals to execute'
        required: false
        type: string
        default: 'clean package --file pom.xml'
      test-report:
        description: 'Whether to generate test reports'
        required: false
        type: boolean
        default: false
      summaries-test-report:
        description: 'Whether to generate test summaries'
        required: false
        type: boolean
        default: false
      pr_number:
        description: 'Pull request number for commenting (when called from PR context)'
        required: false
        type: string
    outputs:
      test-summary:
        description: 'Test summary content'
        value: ${{ jobs.build.outputs.test-summary }}
    secrets:
      token:
        required: true
jobs:
  build:
    runs-on: ubuntu-latest
    outputs:
      test-summary: ${{ steps.capture-summary.outputs.test-summary }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
        env:
          GITHUB_TOKEN: ${{ secrets.token }}

      - name: Set up JDK
        uses: actions/setup-java@v4
        with:
          java-version: ${{ inputs.java_version }}
          distribution: zulu
          cache: maven

      - name: Cache Maven dependencies
        uses: actions/cache@v4
        with:
          path: ~/.m2/repository
          key: maven-${{ runner.os }}-${{ hashFiles('**/pom.xml') }}
          restore-keys: |
            maven-${{ runner.os }}-

      - name: Build with Maven
        run: mvn ${{ inputs.maven_args }}
      
      - name: Generate aggregated Surefire reports
        if: ${{ inputs.test-report || inputs.summaries-test-report }}
        run: |
          # Create directory for aggregated reports
          mvn surefire-report:report-only -B
        
      - name: Parse test results and add to summary
        if: ${{ inputs.summaries-test-report }}
        run: |
          # Create workflow summary with test results
          echo "# ðŸ“Š Test Report Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Count total tests from XML files
          TOTAL_TESTS=0
          TOTAL_FAILURES=0
          TOTAL_ERRORS=0
          TOTAL_SKIPPED=0
          
          # Process all TEST-*.xml files from all modules
          for xml_file in */target/surefire-reports/TEST-*.xml; do
            if [ -f "$xml_file" ]; then
              tests=$(grep -o 'tests="[0-9]*"' "$xml_file" | grep -o '[0-9]*' || echo "0")
              failures=$(grep -o 'failures="[0-9]*"' "$xml_file" | grep -o '[0-9]*' || echo "0")
              errors=$(grep -o 'errors="[0-9]*"' "$xml_file" | grep -o '[0-9]*' || echo "0")
              skipped=$(grep -o 'skipped="[0-9]*"' "$xml_file" | grep -o '[0-9]*' || echo "0")
              
              TOTAL_TESTS=$((TOTAL_TESTS + tests))
              TOTAL_FAILURES=$((TOTAL_FAILURES + failures))
              TOTAL_ERRORS=$((TOTAL_ERRORS + errors))
              TOTAL_SKIPPED=$((TOTAL_SKIPPED + skipped))
            fi
          done
          
          TOTAL_PASSED=$((TOTAL_TESTS - TOTAL_FAILURES - TOTAL_ERRORS - TOTAL_SKIPPED))
          
          # Create summary table
          echo "## Test Results Overview" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Status | Count | Percentage |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|-------|------------|" >> $GITHUB_STEP_SUMMARY
          
          if [ $TOTAL_TESTS -gt 0 ]; then
            PASS_PCT=$(( (TOTAL_PASSED * 100) / TOTAL_TESTS ))
            FAIL_PCT=$(( ((TOTAL_FAILURES + TOTAL_ERRORS) * 100) / TOTAL_TESTS ))
            SKIP_PCT=$(( (TOTAL_SKIPPED * 100) / TOTAL_TESTS ))
            
            echo "| âœ… **Passed** | $TOTAL_PASSED | ${PASS_PCT}% |" >> $GITHUB_STEP_SUMMARY
            echo "| âŒ **Failed** | $((TOTAL_FAILURES + TOTAL_ERRORS)) | ${FAIL_PCT}% |" >> $GITHUB_STEP_SUMMARY
            echo "| â­ï¸ **Skipped** | $TOTAL_SKIPPED | ${SKIP_PCT}% |" >> $GITHUB_STEP_SUMMARY
            echo "| ðŸ“Š **Total** | $TOTAL_TESTS | 100% |" >> $GITHUB_STEP_SUMMARY
          else
            echo "| âš ï¸ **No tests found** | 0 | 0% |" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Add module breakdown
          echo "## Module Test Breakdown" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Module | Tests | Passed | Failed | Skipped |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|-------|--------|--------|---------|" >> $GITHUB_STEP_SUMMARY
          
          for module in api-gateway api-registry api-registry-common; do
            if [ -d "$module/target/surefire-reports" ]; then
              MODULE_TESTS=0
              MODULE_FAILURES=0
              MODULE_ERRORS=0
              MODULE_SKIPPED=0
              
              for xml_file in $module/target/surefire-reports/TEST-*.xml; do
                if [ -f "$xml_file" ]; then
                  tests=$(grep -o 'tests="[0-9]*"' "$xml_file" | grep -o '[0-9]*' || echo "0")
                  failures=$(grep -o 'failures="[0-9]*"' "$xml_file" | grep -o '[0-9]*' || echo "0")
                  errors=$(grep -o 'errors="[0-9]*"' "$xml_file" | grep -o '[0-9]*' || echo "0")
                  skipped=$(grep -o 'skipped="[0-9]*"' "$xml_file" | grep -o '[0-9]*' || echo "0")
                  
                  MODULE_TESTS=$((MODULE_TESTS + tests))
                  MODULE_FAILURES=$((MODULE_FAILURES + failures))
                  MODULE_ERRORS=$((MODULE_ERRORS + errors))
                  MODULE_SKIPPED=$((MODULE_SKIPPED + skipped))
                fi
              done
              
              MODULE_PASSED=$((MODULE_TESTS - MODULE_FAILURES - MODULE_ERRORS - MODULE_SKIPPED))
              MODULE_FAILED=$((MODULE_FAILURES + MODULE_ERRORS))
              
              if [ $MODULE_TESTS -gt 0 ]; then
                echo "| **$module** | $MODULE_TESTS | $MODULE_PASSED | $MODULE_FAILED | $MODULE_SKIPPED |" >> $GITHUB_STEP_SUMMARY
              fi
            fi
          done
          
      - name: Capture test summary for output
        id: capture-summary
        if: ${{ inputs.summaries-test-report }}
        run: |
          # Read the summary file and set as output
          if [ -f "$GITHUB_STEP_SUMMARY" ]; then
            # Create a safe output by encoding the summary
            SUMMARY_CONTENT=$(cat "$GITHUB_STEP_SUMMARY" | base64 -w 0)
            echo "test-summary=$SUMMARY_CONTENT" >> $GITHUB_OUTPUT
          else
            echo "test-summary=" >> $GITHUB_OUTPUT
          fi
          
        
      - name: Comment test summary on PR
        if: ${{ inputs.summaries-test-report && (github.event_name == 'pull_request' || inputs.pr_number) }}
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.token }}
          script: |
            const fs = require('fs');
            
            // Verify we're in a PR context
            console.log('Event name:', context.eventName);
            console.log('Payload keys:', Object.keys(context.payload));
            console.log('Input PR number:', '${{ inputs.pr_number }}');
            
            // Get PR number - from input, pull_request, or issue context
            const prNumber = '${{ inputs.pr_number }}' || 
                           context.payload.pull_request?.number || 
                           context.issue?.number;
            
            if (!prNumber) {
              console.log('Could not determine PR number from any source, skipping PR comment');
              console.log('Available sources:');
              console.log('- Input pr_number:', '${{ inputs.pr_number }}');
              console.log('- Context pull_request number:', context.payload.pull_request?.number);
              console.log('- Context issue number:', context.issue?.number);
              return;
            }
            
            console.log('Using PR Number:', prNumber);
            
            // Add small delay to ensure summary file is fully written
            await new Promise(resolve => setTimeout(resolve, 1000));
            
            // Read the workflow summary that was already generated
            const summaryFile = process.env.GITHUB_STEP_SUMMARY;
            let summaryContent = '';
            
            try {
              if (!summaryFile || !fs.existsSync(summaryFile)) {
                console.log('Summary file does not exist:', summaryFile);
                return;
              }
              
              summaryContent = fs.readFileSync(summaryFile, 'utf8');
              console.log('Successfully read workflow summary');
              console.log('Summary file path:', summaryFile);
              console.log('Summary content length:', summaryContent.length);
              console.log('Summary preview:', summaryContent.substring(0, 200));
              
              // Check if summary actually contains test results (look for specific content)
              if (!summaryContent || summaryContent.trim() === '' || !summaryContent.includes('Test Report Summary')) {
                console.log('Workflow summary is empty or does not contain test results, skipping PR comment');
                console.log('Summary content available:', summaryContent ? 'yes' : 'no');
                console.log('Contains Test Report Summary:', summaryContent.includes('Test Report Summary'));
                return;
              }
              
            } catch (error) {
              console.log('Could not read workflow summary:', error.message);
              console.log('Skipping PR comment since summary is not available');
              return;
            }
            
            // Add PR-specific header and footer to the existing summary
            let comment = `## ðŸ§ª Maven Test Results\n\n`;
            comment += summaryContent;
            comment += `\n\n---\n*This summary was generated by the Maven Test Workflow for Pull Request #${prNumber}*`;
            
            // Post comment to PR
            try {
              const result = await github.rest.issues.createComment({
                issue_number: parseInt(prNumber),
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: comment
              });
              console.log('Successfully posted PR comment with ID:', result.data.id);
            } catch (error) {
              console.log('Failed to post PR comment:', error.message);
              console.log('Error details:', error);
              console.log('Attempting to post to repo:', context.repo.owner + '/' + context.repo.repo);
              console.log('PR number used:', prNumber);
              throw error; // Re-throw to mark step as failed
            }
        
      - name: Upload Surefire reports
        if: ${{ inputs.test-report }}
        uses: actions/upload-artifact@v4
        with:
          name: unit-test-report
          path: target/reports/
          retention-days: 30
          
      - name: Upload individual module reports
        if: ${{ inputs.test-report }}
        uses: actions/upload-artifact@v4
        with:
          name: unit-test-modules-report
          path: |
            */target/site/surefire-report.html
            */target/surefire-reports/
          retention-days: 30
